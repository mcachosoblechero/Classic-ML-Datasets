{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tweet Analyzer Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miguelcachosoblechero/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# NLP Preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# NLP Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Data Modelling - ML\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, recall_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgboost.config_context(verbosity=0) # Silect XGBoost\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Monitoring progress\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "config = {\n",
    "    'val_size': 0.2,\n",
    "    'CV_splits': 5,\n",
    "    'seed': 14,\n",
    "    'n_cores': 16,\n",
    "    'max_features': 2000,\n",
    "    'refresh_embedding': False,\n",
    "    'max_tweet_length': 100,\n",
    "    'embedding_dim': 300,\n",
    "    'num_epochs': 30,\n",
    "    'patience': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Preprocessing and Feature Engineering\n",
    "# Load dataset\n",
    "input_path = \"../input\"\n",
    "raw_tweets_train = pd.read_csv(os.path.join(input_path, \"train.csv\")).drop(['id'], axis=1)\n",
    "raw_tweets_test = pd.read_csv(os.path.join(input_path, \"test.csv\"))\n",
    "\n",
    "# Extract data and labels\n",
    "X_train = raw_tweets_train.drop(['target'], axis=1)\n",
    "y_train = raw_tweets_train.target.values\n",
    "X_test = raw_tweets_test\n",
    "\n",
    "# Use Twitter Tokenizer to tokenize tweets\n",
    "tokenizer = TweetTokenizer()\n",
    "X_train['twitterTokens'] = X_train.apply(lambda x: tokenizer.tokenize(x.text.lower()), axis=1)\n",
    "X_test['twitterTokens'] = X_test.apply(lambda x: tokenizer.tokenize(x.text.lower()), axis=1)\n",
    "\n",
    "# Remove stop words\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "X_train['twitterTokens_noStop'] = X_train.twitterTokens.apply(lambda x: [i for i in x if i not in english_stopwords])\n",
    "X_test['twitterTokens_noStop'] = X_test.twitterTokens.apply(lambda x: [i for i in x if i not in english_stopwords])\n",
    "\n",
    "# Tokenize + Stop Words + BoW\n",
    "CountVec = CountVectorizer(stop_words='english', max_features=config['max_features'])\n",
    "X_train_bow = CountVec.fit_transform(X_train.text)\n",
    "X_test_bow = CountVec.transform(X_test.text)\n",
    "\n",
    "# TfidfVectorizer can be used to perform this action in normal text\n",
    "CountVecTFIDF = TfidfVectorizer(stop_words='english', max_features=config['max_features'])\n",
    "X_train_tfidf = CountVecTFIDF.fit_transform(X_train.text)\n",
    "X_test_tfidf = CountVecTFIDF.transform(X_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for Deep Learning\n",
    "# Tokenize using Keras interface\n",
    "keras_CountVect = Tokenizer()\n",
    "keras_CountVect.fit_on_texts(X_train.text) \n",
    "X_train_tokens = keras_CountVect.texts_to_sequences(X_train.text)\n",
    "X_test_tokens = keras_CountVect.texts_to_sequences(X_test.text)\n",
    "word_index = keras_CountVect.word_index # <- This is effectively a dictionary with all the required words, used later in the Embeddings\n",
    "num_words = len(word_index)+1\n",
    "\n",
    "# Pad the sequences\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=config['max_tweet_length'])\n",
    "X_test_padded = pad_sequences(X_test_tokens, maxlen=config['max_tweet_length'])\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# Divide between train and validation, by shuffling the indices\n",
    "indices = np.arange(X_train_padded.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train_padded = X_train_padded[indices]\n",
    "y_train = y_train[indices]\n",
    "num_validation_samples = int(config['val_size'] * X_train_padded.shape[0])\n",
    "X_train_padded = X_train_padded[:num_validation_samples]\n",
    "y_train_cat = y_train[:num_validation_samples]\n",
    "X_val_padded = X_train_padded[-num_validation_samples:]\n",
    "y_val_cat = y_train[-num_validation_samples:]\n",
    "\n",
    "# Now we are ready to load the associated Embeddings\n",
    "# Download or load from local\n",
    "# if config['refresh_embedding']:\n",
    "#     # Download Google's pre-trained Word2Vec model\n",
    "#     word2vec = api.load('word2vec-google-news-300')\n",
    "#     # Save the model for future reuse\n",
    "#     word2vec.save_word2vec_format('../Word2Vec/word2vec_300.kv')\n",
    "# else:\n",
    "#     word2vec = KeyedVectors.load_word2vec_format('../Word2Vec/word2vec_300.kv')\n",
    "\n",
    "# Associate each index with its embedding\n",
    "# Create an embedding matrix as big as the words available\n",
    "embedding_matrix = np.zeros((num_words, config['embedding_dim']))\n",
    "# For each word in the dictionary, populate the embedding matrix\n",
    "for word, idx in word_index.items():\n",
    "    # If word is in embedding, store\n",
    "    if word in word2vec:\n",
    "        # Store the new embedding vector in the associated position\n",
    "        # Otherwise, leave blank as zero\n",
    "        embedding_matrix[idx] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 2- Data modelling - ML\n",
    "# Select your model\n",
    "# target_model = XGBClassifier(n_estimators=20, max_depth=50, random_state=config['seed'], n_jobs = config['n_cores'])\n",
    "target_model = MultinomialNB()\n",
    "\n",
    "# Train your model\n",
    "target_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "results = target_model.predict(X_test_tfidf)\n",
    "\n",
    "# Store results\n",
    "pd.DataFrame({\"id\": raw_tweets_test.id,\n",
    "              \"target\": results}).set_index(\"id\").to_csv(\"../submission/nlp_submission_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "48/48 [==============================] - 14s 224ms/step - loss: 0.5637 - accuracy: 0.7116 - recall: 0.7148 - auc: 0.7863 - val_loss: 1.0161 - val_accuracy: 0.5131 - val_recall: 0.5171 - val_auc: 0.5136\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 12s 258ms/step - loss: 0.4409 - accuracy: 0.8062 - recall: 0.8068 - auc: 0.8770 - val_loss: 0.9499 - val_accuracy: 0.5204 - val_recall: 0.5197 - val_auc: 0.5213\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 11s 229ms/step - loss: 0.4142 - accuracy: 0.8259 - recall: 0.8265 - auc: 0.8913 - val_loss: 0.9761 - val_accuracy: 0.5217 - val_recall: 0.5223 - val_auc: 0.5230\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 11s 219ms/step - loss: 0.4008 - accuracy: 0.8239 - recall: 0.8246 - auc: 0.8989 - val_loss: 1.1551 - val_accuracy: 0.5171 - val_recall: 0.5171 - val_auc: 0.5170\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 10s 202ms/step - loss: 0.3629 - accuracy: 0.8430 - recall: 0.8469 - auc: 0.9192 - val_loss: 1.1353 - val_accuracy: 0.5237 - val_recall: 0.5230 - val_auc: 0.5208\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 10s 219ms/step - loss: 0.3593 - accuracy: 0.8476 - recall: 0.8449 - auc: 0.9199 - val_loss: 1.0952 - val_accuracy: 0.5394 - val_recall: 0.5394 - val_auc: 0.5332\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 11s 227ms/step - loss: 0.3467 - accuracy: 0.8489 - recall: 0.8476 - auc: 0.9250 - val_loss: 1.3676 - val_accuracy: 0.5217 - val_recall: 0.5243 - val_auc: 0.5158\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 10s 213ms/step - loss: 0.3381 - accuracy: 0.8509 - recall: 0.8574 - auc: 0.9307 - val_loss: 1.2448 - val_accuracy: 0.5315 - val_recall: 0.5322 - val_auc: 0.5286\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 10s 212ms/step - loss: 0.3084 - accuracy: 0.8640 - recall: 0.8640 - auc: 0.9419 - val_loss: 1.3776 - val_accuracy: 0.5427 - val_recall: 0.5414 - val_auc: 0.5332\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 11s 222ms/step - loss: 0.3113 - accuracy: 0.8752 - recall: 0.8758 - auc: 0.9401 - val_loss: 1.4726 - val_accuracy: 0.5263 - val_recall: 0.5276 - val_auc: 0.5291\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 11s 230ms/step - loss: 0.2898 - accuracy: 0.8811 - recall: 0.8824 - auc: 0.9494 - val_loss: 1.6001 - val_accuracy: 0.5269 - val_recall: 0.5283 - val_auc: 0.5245\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 11s 220ms/step - loss: 0.2679 - accuracy: 0.8883 - recall: 0.8903 - auc: 0.9568 - val_loss: 1.5506 - val_accuracy: 0.5276 - val_recall: 0.5289 - val_auc: 0.5261\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 11s 223ms/step - loss: 0.2675 - accuracy: 0.8883 - recall: 0.8870 - auc: 0.9568 - val_loss: 1.4799 - val_accuracy: 0.5243 - val_recall: 0.5243 - val_auc: 0.5326\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 11s 226ms/step - loss: 0.2495 - accuracy: 0.8949 - recall: 0.8949 - auc: 0.9625 - val_loss: 1.5739 - val_accuracy: 0.5151 - val_recall: 0.5151 - val_auc: 0.5293\n",
      "102/102 [==============================] - 6s 54ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the embeddings to use in this training\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            config['embedding_dim'],\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=config['max_tweet_length'],\n",
    "                            trainable=False)\n",
    "\n",
    "# Define trainable model\n",
    "target_model = Sequential()\n",
    "target_model.add(embedding_layer)\n",
    "target_model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "target_model.add(Dense(2, activation='sigmoid'))\n",
    "target_model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy', 'Recall', 'AUC'])\n",
    "\n",
    "target_model.fit(X_train_padded, y_train_cat,\n",
    "        batch_size=32,\n",
    "        epochs=config['num_epochs'],\n",
    "        validation_data=(X_val_padded, y_val_cat),\n",
    "        callbacks=[EarlyStopping(monitor=\"val_accuracy\", patience=config['patience'])]\n",
    "        )\n",
    "\n",
    "# Generate predictions\n",
    "results = target_model.predict(X_test_padded)\n",
    "\n",
    "# Store results\n",
    "pd.DataFrame({\"id\": raw_tweets_test.id,\n",
    "              \"target\": np.argmax(results, axis=1)}).set_index(\"id\").to_csv(\"../submission/nlp_submission_dl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
